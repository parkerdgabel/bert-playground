2025-07-17 12:38:52 | INFO     | training.memory_manager:__init__:82 - Memory Manager initialized:
  Apple Silicon: True
  Unified Memory: 32.00 GB
  Optimal Usage: 64.0%
  Target Memory: 20.48 GB
2025-07-17 12:38:52 | INFO     | training.performance_profiler:__init__:101 - Performance Profiler initialized:
  Apple Silicon: True
  Neural Engine: True
  Thermal Monitoring: True
  Power Monitoring: True
2025-07-17 12:38:52 | INFO     | training.monitoring:__init__:689 - Initialized comprehensive monitoring system
2025-07-17 12:38:52 | INFO     | training.mlx_trainer:_apply_apple_silicon_optimizations:1028 - Applying Apple Silicon optimizations...
2025-07-17 12:38:52 | INFO     | training.mlx_trainer:_apply_apple_silicon_optimizations:1043 - Memory recommendations: Increase batch size for better throughput, Consider larger model if needed
2025-07-17 12:38:52 | INFO     | training.mlx_trainer:__init__:142 - Initialized Production MLX Trainer:
  Model: TitanicClassifier
  Optimization Level: auto
  Batch Size: 16 (effective: 16)
  Learning Rate: 2e-05
  Epochs: 1
  Apple Silicon: True
  MLflow Enabled: False
  Rich Console: True
  Output Dir: test_output
2025-07-17 12:38:52 | INFO     | utils.logging_config:__enter__:128 - ============================================================
2025-07-17 12:38:52 | INFO     | utils.logging_config:__enter__:129 - Starting experiment: mlx_training
2025-07-17 12:38:52 | INFO     | utils.logging_config:__enter__:130 - Start time: 2025-07-17 12:38:52.194838
2025-07-17 12:38:52 | INFO     | utils.logging_config:__enter__:131 - ============================================================
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:99 - Hyperparameters:
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   epochs: 1
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   batch_size: 16
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   learning_rate: 2e-05
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   warmup_steps: 500
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   max_steps: None
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   model_name: answerdotai/ModernBERT-base
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   model_type: modernbert
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   max_length: 256
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   num_labels: None
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   train_path: data/titanic/train.csv
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   val_path: data/titanic/val.csv
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   test_path: None
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   target_column: None
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   optimizer: adamw
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   lr_schedule: cosine_warmup
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   loss_function: cross_entropy
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   optimization_level: auto
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   memory: {'enable_memory_profiling': True, 'memory_limit_gb': None, 'dynamic_batch_sizing': True, 'min_batch_size': 4, 'max_batch_size': 128, 'memory_check_interval': 100, 'unified_memory_fraction': 0.8, 'enable_memory_pool': True, 'force_garbage_collection': True, 'gc_interval': 500}
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   mlx_optimization: {'enable_lazy_evaluation': True, 'eval_frequency': 10, 'enable_gradient_checkpointing': False, 'gradient_accumulation_steps': 1, 'max_grad_norm': 1.0, 'device_placement_strategy': 'auto', 'enable_multi_device': False, 'mixed_precision': False, 'precision_dtype': 'float32', 'enable_jit': True, 'optimize_memory_layout': True}
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   monitoring: {'enable_mlflow': False, 'experiment_name': 'mlx_training', 'run_name': None, 'tracking_uri': None, 'log_level': 'INFO', 'log_to_file': True, 'log_file_path': 'test_output/training.log', 'enable_rich_console': True, 'log_frequency': 10, 'eval_frequency': 500, 'save_frequency': 1000, 'enable_progress_bar': True, 'progress_bar_style': 'rich', 'track_gradients': False, 'track_weights': False, 'track_memory': True, 'track_performance': True}
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   checkpoint: {'enable_checkpointing': True, 'checkpoint_dir': 'test_output/checkpoints', 'checkpoint_frequency': 1000, 'save_optimizer_state': True, 'save_scheduler_state': True, 'save_random_state': True, 'save_model_weights': True, 'max_checkpoints_to_keep': 5, 'save_best_model': True, 'best_model_metric': 'val_accuracy', 'best_model_mode': 'max', 'auto_resume': True, 'resume_from_checkpoint': None, 'use_safetensors': True, 'compress_checkpoints': False}
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   evaluation: {'eval_during_training': True, 'eval_steps': 500, 'eval_strategy': 'steps', 'primary_metric': 'accuracy', 'metrics_to_compute': ['accuracy', 'precision', 'recall', 'f1', 'auc'], 'enable_early_stopping': True, 'early_stopping_patience': 10, 'early_stopping_threshold': 0.001, 'early_stopping_metric': 'val_loss', 'early_stopping_mode': 'min', 'validation_split': 0.2, 'validation_batch_size': None, 'test_at_end': True, 'generate_predictions': True, 'save_predictions': True}
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   advanced: {'label_smoothing': 0.0, 'dropout_rate': 0.1, 'weight_decay': 0.01, 'enable_augmentation': True, 'augmentation_probability': 0.5, 'enable_curriculum_learning': False, 'curriculum_strategy': 'difficulty', 'curriculum_pace': 0.1, 'enable_ensembling': False, 'ensemble_size': 1, 'ensemble_strategy': 'averaging', 'enable_distillation': False, 'teacher_model_path': None, 'distillation_temperature': 3.0, 'distillation_alpha': 0.5, 'enable_hpo': False, 'hpo_backend': 'optuna', 'hpo_trials': 50, 'hpo_metric': 'val_accuracy'}
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   seed: 42
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   deterministic: True
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   output_dir: test_output
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   experiment_name: None
2025-07-17 12:38:52 | INFO     | utils.logging_config:log_hyperparameters:101 -   run_name: None
2025-07-17 12:38:52 | INFO     | training.config:_auto_configure_optimization:310 - Auto-configuring optimization level...
2025-07-17 12:38:52 | INFO     | training.config:update_from_dataset:616 - Auto-configured optimization level: OptimizationLevel.DEVELOPMENT
2025-07-17 12:38:52 | INFO     | training.mlx_trainer:train:650 - Training Configuration:
  Steps per epoch: 56
  Total steps: 56
  Effective batch size: 16
  Warmup steps: 500
2025-07-17 12:38:52 | INFO     | training.monitoring:start_training:699 - Started training monitoring: 1 epochs, 56 steps/epoch
2025-07-17 12:39:03 | INFO     | training.performance_profiler:_log_metrics:414 - Step 10: Time=1.118s, Throughput=14.3 samples/s, Memory=80.9%
2025-07-17 12:39:14 | INFO     | training.performance_profiler:_log_metrics:414 - Step 20: Time=1.130s, Throughput=14.2 samples/s, Memory=80.7%
2025-07-17 12:39:25 | INFO     | training.performance_profiler:_log_metrics:414 - Step 30: Time=1.094s, Throughput=14.6 samples/s, Memory=81.0%
2025-07-17 12:39:36 | INFO     | training.performance_profiler:_log_metrics:414 - Step 40: Time=1.080s, Throughput=14.8 samples/s, Memory=80.9%
2025-07-17 12:39:47 | INFO     | training.performance_profiler:_log_metrics:414 - Step 50: Time=1.087s, Throughput=14.7 samples/s, Memory=80.9%
2025-07-17 12:39:55 | INFO     | training.mlx_trainer:train:686 - Epoch 1/1 completed: Loss=0.6866, Time=62.8s
2025-07-17 12:39:55 | INFO     | training.mlx_trainer:_final_evaluation:876 - Running final validation evaluation
2025-07-17 12:39:55 | INFO     | training.mlx_trainer:evaluate:472 - Starting final_validation evaluation
2025-07-17 12:39:59 | INFO     | training.mlx_trainer:evaluate:565 - Final_validation Results: Loss=0.6508, Acc=0.6573, F1=0.0000
2025-07-17 12:40:00 | INFO     | models.modernbert:save_pretrained:329 - Model saved to test_output/checkpoints/final/bert_model
2025-07-17 12:40:00 | INFO     | classification:save_pretrained:421 - TitanicClassifier saved to test_output/checkpoints/final
2025-07-17 12:40:01 | INFO     | training.mlx_trainer:_save_checkpoint:935 - Checkpoint saved: test_output/checkpoints/final
2025-07-17 12:40:01 | INFO     | training.memory_manager:save_memory_report:417 - Memory report saved to: test_output/memory_report.json
2025-07-17 12:40:01 | INFO     | training.performance_profiler:save_performance_report:561 - Performance report saved to: test_output/performance_report.json
2025-07-17 12:40:01 | INFO     | training.monitoring:end_training:894 - Training monitoring ended with status: FINISHED
2025-07-17 12:40:01 | INFO     | training.monitoring:end_training:895 - Total time: 69.1s, Steps: 56
2025-07-17 12:40:01 | INFO     | training.memory_manager:save_memory_report:417 - Memory report saved to: test_output/memory_report.json
2025-07-17 12:40:01 | INFO     | training.performance_profiler:save_performance_report:561 - Performance report saved to: test_output/performance_report.json
2025-07-17 12:40:01 | INFO     | training.mlx_trainer:_save_advanced_reports:1069 - Advanced profiling reports saved successfully
2025-07-17 12:40:01 | INFO     | training.mlx_trainer:train:745 - Training completed in 67.1s
Best metric: -inf at step 0
2025-07-17 12:40:01 | INFO     | utils.logging_config:__exit__:142 - ============================================================
2025-07-17 12:40:01 | INFO     | utils.logging_config:__exit__:143 - Experiment completed: mlx_training
2025-07-17 12:40:01 | INFO     | utils.logging_config:__exit__:144 - End time: 2025-07-17 12:40:01.250653
2025-07-17 12:40:01 | INFO     | utils.logging_config:__exit__:145 - Duration: 0:01:09.055815
2025-07-17 12:40:01 | SUCCESS  | utils.logging_config:__exit__:151 - Experiment completed successfully!
2025-07-17 12:40:01 | INFO     | utils.logging_config:__exit__:153 - ============================================================
