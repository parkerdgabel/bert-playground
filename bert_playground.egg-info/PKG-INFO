Metadata-Version: 2.4
Name: bert-playground
Version: 0.1.0
Summary: MLX-based ModernBERT implementation for Kaggle competitions
Requires-Python: <3.11,>=3.10
Description-Content-Type: text/markdown
Requires-Dist: mlx<0.25.0,>=0.22.0
Requires-Dist: mlx-data>=0.0.7
Requires-Dist: mlx-lm>=0.20.0
Requires-Dist: mlx-vlm<0.4.0,>=0.1.21
Requires-Dist: mlx-embeddings@ git+https://github.com/Blaizzy/mlx-embeddings.git
Requires-Dist: numba>=0.57.0
Requires-Dist: transformers[sentencepiece]>=4.44.0
Requires-Dist: pandas>=2.2.0
Requires-Dist: numpy>=1.26.0
Requires-Dist: scikit-learn>=1.5.0
Requires-Dist: tqdm>=4.67.0
Requires-Dist: kaggle>=1.6.0
Requires-Dist: mlflow>=2.19.0
Requires-Dist: matplotlib>=3.9.0
Requires-Dist: seaborn>=0.13.0
Requires-Dist: rich>=13.10.0
Requires-Dist: loguru>=0.7.2
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: typer>=0.15.0
Requires-Dist: click>=8.1.0
Requires-Dist: psutil>=5.9.0
Requires-Dist: pyyaml>=6.0.2
Provides-Extra: dev
Requires-Dist: pytest>=8.0.0; extra == "dev"
Requires-Dist: pytest-cov>=6.0.0; extra == "dev"
Requires-Dist: pytest-mock>=3.14.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.25.0; extra == "dev"
Requires-Dist: pytest-xdist>=3.6.1; extra == "dev"
Requires-Dist: ruff>=0.8.0; extra == "dev"
Requires-Dist: mypy>=1.13.0; extra == "dev"
Requires-Dist: ipykernel>=6.29.0; extra == "dev"
Requires-Dist: black>=24.10.0; extra == "dev"

# MLX ModernBERT for Kaggle

Optimized ModernBERT implementation using Apple's MLX framework for solving Kaggle competitions with a text-based approach.

## Features

- ðŸš€ **MLX Optimized**: Fully optimized for Apple Silicon with MLX
- ðŸ“Š **Text-Based Tabular**: Converts tabular data to natural language
- ðŸ”„ **Data Augmentation**: Multiple text templates for better generalization
- ðŸ“ˆ **MLflow Integration**: Complete experiment tracking
- ðŸŽ¯ **Production Ready**: Unified CLI with multiple configurations
- âš¡ **High Performance**: Efficient data pipeline with prefetching

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd bert-playground

# Install dependencies using uv
uv sync
```

## Quick Start

### Production Training

```bash
# Standard training (recommended)
uv run python mlx_bert_cli.py train \
    --train data/titanic/train.csv \
    --val data/titanic/val.csv \
    --batch-size 32 \
    --epochs 5
```

### Using Configuration Files

```bash
# Use pre-defined configurations
uv run python mlx_bert_cli.py train \
    --train data/titanic/train.csv \
    --val data/titanic/val.csv \
    --config configs/production.json
```

### Generate Predictions

```bash
# Create Kaggle submission
uv run python mlx_bert_cli.py predict \
    --test data/titanic/test.csv \
    --checkpoint output/run_*/best_model_accuracy \
    --output submission.csv
```

### Benchmark Performance

```bash
# Test MLX performance
uv run python mlx_bert_cli.py benchmark \
    --batch-size 64 \
    --seq-length 256 \
    --steps 20
```

## Available Configurations

- **quick**: Fast testing (1 epoch, batch size 64)
- **standard**: Balanced training (5 epochs, batch size 32)
- **thorough**: Extended training (10 epochs, batch size 16)
- **mlx_optimized**: Optimized for MLX (5 epochs, batch size 64)

## CLI Commands

### Training
```bash
uv run python mlx_bert_cli.py train --help
```

### Prediction
```bash
uv run python mlx_bert_cli.py predict --help
```

### Benchmarking
```bash
uv run python mlx_bert_cli.py benchmark --help
```

### System Info
```bash
uv run python mlx_bert_cli.py info
```

## Project Structure

```
bert-playground/
â”œâ”€â”€ mlx_bert_cli.py          # Main CLI interface
â”œâ”€â”€ models/                  # Model implementations
â”‚   â”œâ”€â”€ modernbert_optimized.py
â”‚   â””â”€â”€ classification_head.py
â”œâ”€â”€ data/                    # Data processing
â”‚   â”œâ”€â”€ optimized_loader.py
â”‚   â””â”€â”€ text_templates.py
â”œâ”€â”€ training/                # Training logic
â”‚   â””â”€â”€ trainer_v2.py
â”œâ”€â”€ utils/                   # Utilities
â”‚   â”œâ”€â”€ logging_config.py
â”‚   â””â”€â”€ mlflow_utils.py
â””â”€â”€ configs/                 # Configuration files
    â””â”€â”€ production.json
```

## Performance

Expected performance on Apple Silicon:
- **M1/M2**: 15-30 samples/second
- **Batch size 32**: ~1.5-2.0 seconds/step
- **Batch size 64**: ~2.5-3.5 seconds/step

## Tips

1. Use larger batch sizes (32-64) for better MLX performance
2. Enable data augmentation for improved accuracy
3. Use MLflow to track experiments
4. Adjust learning rate based on batch size

## License

MIT License
