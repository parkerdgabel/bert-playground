# Configuration for saving only the best model during training
# This reduces storage usage by skipping regular checkpoints

# Basic training parameters
epochs: 3
batch_size: 32
learning_rate: 2e-5
warmup_steps: 100

# Model configuration
model_name: "answerdotai/ModernBERT-base"
max_length: 256

# Checkpoint configuration - key setting for best model only
checkpoint:
  enable_checkpointing: true
  save_best_model: true
  save_best_only: true  # This is the new option
  best_model_metric: "val_accuracy"
  best_model_mode: "max"
  checkpoint_frequency: 500  # Ignored when save_best_only is true
  max_checkpoints_to_keep: 3

# Evaluation configuration
evaluation:
  eval_during_training: true
  eval_steps: 100
  primary_metric: "accuracy"
  enable_early_stopping: true
  early_stopping_patience: 5

# Monitoring configuration
monitoring:
  enable_mlflow: true
  log_frequency: 50
  enable_rich_console: true

# MLX optimization
mlx_optimization:
  gradient_accumulation_steps: 1
  enable_jit: true
  mixed_precision: false

# Memory management
memory:
  dynamic_batch_sizing: true
  unified_memory_fraction: 0.8
  enable_memory_profiling: true