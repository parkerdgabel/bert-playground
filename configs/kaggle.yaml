# Kaggle competition optimized configuration
optimizer:
  type: adamw
  learning_rate: 1.0e-5
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8
  max_grad_norm: 1.0
  gradient_clip_val: 1.0

scheduler:
  type: cosine_with_restarts
  warmup_ratio: 0.1
  warmup_steps: 0  # Will be calculated from warmup_ratio
  num_cycles: 0.5
  num_restarts: 2

data:
  batch_size: 64
  eval_batch_size: 128
  num_workers: 8
  prefetch_size: 8
  shuffle_train: true
  drop_last: true
  pin_memory: true
  augment_train: true
  augmentation_prob: 0.5

training:
  num_epochs: 15
  max_steps: -1
  gradient_accumulation_steps: 2
  mixed_precision: true
  eval_strategy: steps
  eval_steps: 250
  eval_delay: 0
  logging_steps: 50
  log_level: info
  report_to: ["mlflow", "tensorboard"]
  save_strategy: best
  save_steps: 250
  save_total_limit: 5
  save_best_only: true
  best_metric: eval_auc
  best_metric_mode: max
  early_stopping: true
  early_stopping_patience: 5
  early_stopping_threshold: 0.0001
  label_smoothing: 0.1
  dropout_rate: 0.1

environment:
  output_dir: output/kaggle
  experiment_name: kaggle_competition
  seed: 42
  device: gpu
  gradient_checkpointing: true
  memory_efficient_attention: true
  debug_mode: false
  profile: false
  mlflow_tags:
    competition: kaggle
    framework: mlx
    strategy: ensemble