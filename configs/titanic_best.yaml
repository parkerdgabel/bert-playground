# Titanic Best Classifier Configuration
# Optimized configuration for maximum performance on the Titanic dataset
# Using ModernBERT with advanced training techniques

# Model configuration
model_name: answerdotai/ModernBERT-base
model_type: modernbert
max_length: 384  # Longer sequences for richer context
num_labels: 2   # Binary classification

# Training hyperparameters
epochs: 15
batch_size: 32
learning_rate: 3e-05  # Slightly higher for faster convergence
warmup_steps: 50     # 10% of total steps for 15 epochs
max_steps: null       # Train for full epochs

# Optimizer configuration
optimizer: adamw
lr_schedule: cosine_warmup

# Loss function with class imbalance handling
loss_function: cross_entropy

# Advanced features
advanced:
  dropout_rate: 0.15          # Slightly higher dropout for regularization
  weight_decay: 0.01          # L2 regularization
  enable_augmentation: true   # Text augmentation
  augmentation_probability: 0.3
  enable_curriculum_learning: false
  enable_ensembling: false
  enable_distillation: false
  enable_hpo: false

# Memory optimization for Apple Silicon
memory:
  enable_memory_profiling: true
  memory_limit_gb: null        # Auto-detect
  dynamic_batch_sizing: true
  min_batch_size: 8
  max_batch_size: 64
  memory_check_interval: 100
  unified_memory_fraction: 0.85  # Use more unified memory
  enable_memory_pool: true
  force_garbage_collection: true
  gc_interval: 500

# MLX-specific optimizations
mlx_optimization:
  enable_lazy_evaluation: true
  eval_frequency: 10
  enable_gradient_checkpointing: false  # Not needed for base model
  gradient_accumulation_steps: 2        # Effective batch size of 64
  max_grad_norm: 1.0
  device_placement_strategy: auto
  enable_multi_device: false
  mixed_precision: false
  precision_dtype: float32
  enable_jit: true
  optimize_memory_layout: true

# Comprehensive monitoring with MLflow
monitoring:
  enable_mlflow: true
  experiment_name: titanic_best_classifier
  run_name: null  # Auto-generate based on timestamp
  tracking_uri: null  # Use default
  log_level: INFO
  log_to_file: true
  log_file_path: null  # Auto-generate
  enable_rich_console: true
  log_frequency: 10
  eval_frequency: 50    # More frequent evaluation
  save_frequency: 100
  enable_progress_bar: true
  progress_bar_style: rich
  track_gradients: true   # Track gradient norms
  track_weights: false    # Don't track weights (too much data)
  track_memory: true
  track_performance: true

# Checkpointing strategy
checkpoint:
  enable_checkpointing: true
  checkpoint_dir: null  # Auto-generate
  checkpoint_frequency: 100
  save_optimizer_state: true
  save_scheduler_state: true
  save_random_state: true
  save_model_weights: true
  max_checkpoints_to_keep: 5
  save_best_model: true
  save_best_only: true  # Save only the best model to reduce storage usage
  best_model_metric: val_accuracy
  best_model_mode: max
  auto_resume: true
  resume_from_checkpoint: null
  use_safetensors: true
  compress_checkpoints: false

# Evaluation configuration
evaluation:
  eval_during_training: true
  eval_steps: 50              # Evaluate every 50 steps
  eval_strategy: steps
  primary_metric: accuracy
  metrics_to_compute:
    - accuracy
    - precision
    - recall
    - f1
    - auc
  enable_early_stopping: true
  early_stopping_patience: 5   # Stop if no improvement for 5 evaluations
  early_stopping_threshold: 0.001
  early_stopping_metric: val_loss
  early_stopping_mode: min
  validation_split: 0.2       # If no validation set provided
  validation_batch_size: 64   # Larger batch for evaluation
  test_at_end: true
  generate_predictions: true
  save_predictions: true

# Data loading configuration
train_path: data/titanic/train.csv
val_path: data/titanic/val.csv
test_path: null
target_column: Survived

# Output configuration
output_dir: ${OUTPUT_DIR:./output/titanic_best}  # Environment variable with default
experiment_name: titanic_best_classifier
run_name: null  # Auto-generate

# Random seed for reproducibility
seed: 42
deterministic: true

# Optimization level
optimization_level: production  # Maximum performance

# Additional notes and metadata
metadata:
  description: |
    Optimized configuration for Titanic dataset classification.
    This configuration uses:
    - ModernBERT base model with longer sequences (384 tokens)
    - Cosine learning rate schedule with warmup
    - Gradient accumulation for larger effective batch size
    - Label smoothing for better generalization
    - Comprehensive evaluation metrics
    - Early stopping to prevent overfitting
    - MLflow tracking for experiment management
    - Best model only saving to reduce storage usage

  expected_performance:
    validation_accuracy: 0.83-0.86
    training_time: ~10-15 minutes on M1/M2 Mac
    memory_usage: ~4-6 GB