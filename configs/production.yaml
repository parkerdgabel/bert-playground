# Production Training Configuration
# Standard settings for production model training

# Model configuration
model_name: answerdotai/ModernBERT-base

# Training hyperparameters
batch_size: 32
learning_rate: 2.0e-05
num_epochs: 5
max_length: 256
warmup_steps: 100
gradient_accumulation: 1
num_workers: 4
augment: true
experiment_name: titanic_production
use_mlx_embeddings: false
tokenizer_backend: auto
configurations:
  quick:
    description: Quick test run
    num_epochs: 1

# Training hyperparameters
    batch_size: 64
    warmup_steps: 50
  standard:
    description: Standard training configuration
    num_epochs: 5

# Training hyperparameters
    batch_size: 32
    learning_rate: 2.0e-05
    warmup_steps: 100
  thorough:
    description: Thorough training with smaller batch size
    num_epochs: 10

# Training hyperparameters
    batch_size: 16
    learning_rate: 1.0e-05
    warmup_steps: 200
    gradient_accumulation: 2
  mlx_optimized:
    description: Optimized for MLX performance
    num_epochs: 5

# Training hyperparameters
    batch_size: 64
    learning_rate: 3.0e-05
    warmup_steps: 100
    num_workers: 8
    augment: true
  mlx_embeddings:
    description: Using MLX embeddings backend
    use_mlx_embeddings: true
    tokenizer_backend: mlx
# Model configuration
    model_name: mlx-community/answerdotai-ModernBERT-base-4bit
    num_epochs: 5

# Training hyperparameters
    batch_size: 32
    learning_rate: 2.0e-05
    warmup_steps: 100
  mlx_embeddings_large:
    description: Using MLX embeddings with large model
    use_mlx_embeddings: true
    tokenizer_backend: mlx
# Model configuration
    model_name: mlx-community/answerdotai-ModernBERT-large-4bit
    num_epochs: 5

# Training hyperparameters
    batch_size: 16
    learning_rate: 1.0e-05
    warmup_steps: 150
    gradient_accumulation: 2
