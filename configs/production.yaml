# Production configuration for optimal performance
optimizer:
  type: adamw
  learning_rate: 2.0e-5
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.0e-8
  max_grad_norm: 1.0

scheduler:
  type: cosine
  warmup_ratio: 0.05
  warmup_steps: 0  # Will be calculated from warmup_ratio
  num_cycles: 0.5

data:
  batch_size: 32
  eval_batch_size: 64
  num_workers: 8
  prefetch_size: 2  # Number of batches to prefetch
  shuffle_train: true
  drop_last: false
  pin_memory: true
  augment_train: true
  augmentation_prob: 0.3
  # MLX-specific optimizations
  mlx_prefetch_size: 2  # Enable async prefetching
  mlx_tokenizer_chunk_size: 100  # Process texts in chunks

training:
  num_epochs: 10
  max_steps: -1
  gradient_accumulation_steps: 1
  mixed_precision: true
  eval_strategy: steps
  eval_steps: 500
  eval_delay: 0
  logging_steps: 50  # Reduced from 100 for better monitoring
  log_level: info
  report_to: ["mlflow"]
  save_strategy: best
  save_steps: 500
  save_total_limit: 3
  save_best_only: true
  best_metric: eval_loss
  best_metric_mode: min
  early_stopping: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.0001
  label_smoothing: 0.0
  dropout_rate: 0.1

environment:
  output_dir: output/production
  experiment_name: production
  seed: 42
  device: gpu
  gradient_checkpointing: true
  memory_efficient_attention: true
  debug_mode: false
  profile: false
  mlflow_tags:
    environment: production
    framework: mlx

# Callback-specific optimizations
callbacks:
  lr_scheduler:
    verbose: false  # Disable per-step logging
    update_freq: step
  progress_bar:
    update_freq: 10  # Update every 10 batches instead of every batch
    show_batch_progress: true
    show_epoch_progress: true
  mlflow:
    log_every_n_steps: 50  # Log to MLflow every 50 steps
    log_model_checkpoints: false  # Only log final/best model
  metrics_logger:
    save_format: json
    plot_freq: epoch  # Only plot at epoch end
    aggregate_metrics: false