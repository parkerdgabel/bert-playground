# k-bert.yaml - Project Configuration Template
# This is the primary way to configure k-bert training runs

name: my-bert-project
competition: titanic  # Optional: for competition-specific defaults
description: BERT model for Kaggle competition
version: "1.0"

# Model configuration
models:
  default_model: answerdotai/ModernBERT-base
  default_architecture: modernbert
  use_mlx_embeddings: true
  use_lora: false
  lora_preset: balanced  # minimal, balanced, or aggressive
  cache_dir: ~/.k-bert/models
  
  # Task-specific head configuration
  head:
    type: binary_classification  # or multiclass, multilabel, regression, etc.
    num_labels: 2
    dropout_prob: 0.1

# Data configuration  
data:
  # Data paths (relative to project root)
  train_path: data/train.csv
  val_path: data/val.csv
  test_path: data/test.csv
  
  # Processing settings
  max_length: 256
  batch_size: 32
  eval_batch_size: 64  # Usually 2x batch_size
  num_workers: 4
  prefetch_size: 4
  use_pretokenized: true
  tokenizer_backend: auto  # auto, mlx, or huggingface
  
  # Augmentation settings (optional)
  augmentation_mode: moderate  # none, light, moderate, heavy
  
  # MLX-specific settings
  mlx_prefetch_size: null  # Set to 0 if using compilation
  mlx_tokenizer_chunk_size: 100

# Training configuration
training:
  # Basic settings
  epochs: 5  # or use default_epochs
  default_epochs: 5
  learning_rate: 2e-5  # or use default_learning_rate
  default_learning_rate: 2e-5
  default_batch_size: 32
  
  # Output settings
  output_dir: ./outputs
  save_best_only: true
  save_steps: 500
  eval_steps: 100
  logging_steps: 10
  
  # Training control
  early_stopping_patience: 3
  gradient_accumulation_steps: 1
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  seed: 42
  mixed_precision: true
  use_compilation: true
  label_smoothing: 0.0
  
  # Advanced settings
  gradient_checkpointing: false
  report_to: ["mlflow"]  # Can include: mlflow, tensorboard, wandb

# MLflow configuration
mlflow:
  auto_log: true
  tracking_uri: file://./mlruns
  default_experiment: my-bert-experiments
  experiment_name: null  # Override default_experiment
  log_models: true
  log_metrics: true

# Kaggle-specific settings (optional)
kaggle:
  competition_name: titanic
  auto_download: true
  auto_submit: false
  submission_message: "Submitted by k-bert"
  
# Logging configuration
logging:
  level: INFO
  format: structured
  file_output: true
  file_dir: ./logs
  rotation: "500 MB"
  retention: "30 days"

# Experiment definitions (optional)
# These allow you to define multiple training configurations
experiments:
  - name: quick_test
    description: Quick test with minimal epochs
    config:
      training:
        epochs: 1
        save_best_only: false
      data:
        batch_size: 8
        
  - name: full_training
    description: Full training with optimal hyperparameters
    config:
      training:
        epochs: 10
        learning_rate: 1e-5
        warmup_ratio: 0.2
      data:
        batch_size: 64
        gradient_accumulation_steps: 2
        
  - name: lora_experiment
    description: Efficient training with LoRA
    config:
      models:
        use_lora: true
        lora_preset: aggressive
      training:
        epochs: 8
        learning_rate: 5e-5

# Custom component configurations (optional)
# These are used when you have custom heads, augmenters, etc.
components:
  custom_head:
    type: my_custom_head
    config:
      hidden_size: 768
      dropout: 0.2
      
  custom_augmenter:
    type: my_augmenter
    config:
      strength: 0.5
      techniques: ["paraphrase", "backtranslation"]