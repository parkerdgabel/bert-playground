2025-07-17 10:18:22.389 | INFO     | utils.mlx_patch:apply_mlx_patches:77 - Applying MLX compatibility patches...
2025-07-17 10:18:22.389 | INFO     | utils.mlx_patch:patch_mlx_astype:72 - Applied MLX astype() compatibility patch
2025-07-17 10:18:22.389 | INFO     | utils.mlx_patch:apply_mlx_patches:79 - MLX compatibility patches applied successfully
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.

MLX Unified Training System
============================================================

Loading data...
2025-07-17 10:18:26.420 | INFO     | data.mlx_dataloader:__init__:82 - Loading tokenizer: answerdotai/ModernBERT-base (backend: auto)
2025-07-17 10:18:26.420 | INFO     | embeddings.mlx_adapter:_load_mlx_embeddings:55 - Loading MLX embeddings model: mlx-community/answerdotai-ModernBERT-base-4bit
Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 38479.85it/s]
2025-07-17 10:18:26.733 | INFO     | embeddings.mlx_adapter:_load_mlx_embeddings:58 - Successfully loaded MLX embeddings model
2025-07-17 10:18:26.733 | INFO     | embeddings.tokenizer_wrapper:_initialize_backend:59 - Using MLX embeddings tokenizer
2025-07-17 10:18:26.737 | DEBUG    | data.mlx_dataloader:_analyze_csv:133 - CSV columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']
2025-07-17 10:18:26.737 | DEBUG    | data.mlx_dataloader:_analyze_csv:134 - Text columns: ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']
2025-07-17 10:18:26.737 | DEBUG    | data.mlx_dataloader:_analyze_csv:135 - Has labels: True
2025-07-17 10:18:26.737 | INFO     | data.mlx_dataloader:_preprocess_data:139 - Preprocessing data...
2025-07-17 10:18:26.898 | INFO     | data.mlx_dataloader:_preprocess_data:173 - Preprocessed 891 samples
2025-07-17 10:18:26.898 | INFO     | data.mlx_dataloader:__init__:104 - Initialized KaggleDataLoader: 891 samples, 27 batches
2025-07-17 10:18:26.898 | INFO     | data.mlx_dataloader:__init__:82 - Loading tokenizer: answerdotai/ModernBERT-base (backend: auto)
2025-07-17 10:18:26.898 | INFO     | embeddings.mlx_adapter:_load_mlx_embeddings:55 - Loading MLX embeddings model: mlx-community/answerdotai-ModernBERT-base-4bit
Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 7813.05it/s]
2025-07-17 10:18:27.081 | INFO     | embeddings.mlx_adapter:_load_mlx_embeddings:58 - Successfully loaded MLX embeddings model
2025-07-17 10:18:27.081 | INFO     | embeddings.tokenizer_wrapper:_initialize_backend:59 - Using MLX embeddings tokenizer
2025-07-17 10:18:27.083 | DEBUG    | data.mlx_dataloader:_analyze_csv:133 - CSV columns: ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']
2025-07-17 10:18:27.083 | DEBUG    | data.mlx_dataloader:_analyze_csv:134 - Text columns: ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Embarked']
2025-07-17 10:18:27.083 | DEBUG    | data.mlx_dataloader:_analyze_csv:135 - Has labels: True
2025-07-17 10:18:27.083 | INFO     | data.mlx_dataloader:_preprocess_data:139 - Preprocessing data...
2025-07-17 10:18:27.117 | INFO     | data.mlx_dataloader:_preprocess_data:173 - Preprocessed 178 samples
2025-07-17 10:18:27.117 | INFO     | data.mlx_dataloader:__init__:104 - Initialized KaggleDataLoader: 178 samples, 2 batches
✓ Loaded ~896 training samples (28 batches)
✓ Loaded ~192 validation samples (3 batches)
2025-07-17 10:18:27.118 | INFO     | training.config:_validate_config:373 - Configuration validation passed
                            Training Configuration                             
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Parameter             ┃ Value                                               ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ Model                 │ answerdotai/ModernBERT-base                         │
│ Model Type            │ base                                                │
│ Output Directory      │ output/standard_20250717_101821/run_20250717_101826 │
│ MLX Embeddings        │ Disabled                                            │
│ Tokenizer Backend     │ auto                                                │
│ Batch Size            │ 32                                                  │
│ Learning Rate         │ 2e-05                                               │
│ Epochs                │ 5                                                   │
│ Gradient Accumulation │ 1                                                   │
│ MLflow                │ Enabled                                             │
│ Early Stopping        │ 3                                                   │
└───────────────────────┴─────────────────────────────────────────────────────┘
2025-07-17 10:18:27.123 | INFO     | models.factory:create_model:113 - Created standard ModernBERT model
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /Users/parkergabel/PycharmProjects/bert-playground/mlx_bert_cli.py:292 in    │
│ train                                                                        │
│                                                                              │
│   289 │   │   │   # Create base model                                        │
│   290 │   │   │   bert_model = create_model("standard")                      │
│   291 │   │   │   model_desc = "OptimizedModernBertMLX"                      │
│ ❱ 292 │   │   │   model = TitanicClassifier(bert_model)                      │
│   293 │                                                                      │
│   294 │   console.print(f"[green]✓ Created {model_desc} model[/green]")      │
│   295                                                                        │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │                 augment = True                                           │ │
│ │              batch_size = 32                                             │ │
│ │       batch_size_config = 32                                             │ │
│ │              bert_model = ModernBertModel(                               │ │
│ │                             (embeddings): OptimizedEmbeddings(           │ │
│ │                           │   (word_embeddings): Embedding(50368, 768)   │ │
│ │                           │   (position_embeddings): Embedding(8192,     │ │
│ │                           768)                                           │ │
│ │                           │   (token_type_embeddings): Embedding(2, 768) │ │
│ │                           │   (LayerNorm): LayerNorm(768, eps=1e-12,     │ │
│ │                           affine=True)                                   │ │
│ │                           │   (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                             )                                            │ │
│ │                             (encoder): Sequential(                       │ │
│ │                           │   (layers.0): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.1): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.2): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.3): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.4): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.5): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.6): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.7): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.8): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.9): TransformerBlock(              │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.10): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.11): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.12): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.13): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.14): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.15): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.16): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.17): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.18): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.19): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.20): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                           │   (layers.21): TransformerBlock(             │ │
│ │                           │     (attention): FusedMultiHeadAttention(    │ │
│ │                           │   │   (qkv_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=2304, bias=True)                   │ │
│ │                           │   │   (out_proj): Linear(input_dims=768,     │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (dropout):                             │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (attention_norm): LayerNorm(768,         │ │
│ │                           eps=1e-12, affine=True)                        │ │
│ │                           │     (mlp): Sequential(                       │ │
│ │                           │   │   (layers.0): Linear(input_dims=768,     │ │
│ │                           output_dims=3072, bias=True)                   │ │
│ │                           │   │   (layers.1): GELU()                     │ │
│ │                           │   │   (layers.2):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │   │   (layers.3): Linear(input_dims=3072,    │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   │   (layers.4):                            │ │
│ │                           Dropout(p=0.09999999999999998)                 │ │
│ │                           │     )                                        │ │
│ │                           │     (mlp_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                           affine=True)                                   │ │
│ │                           │   )                                          │ │
│ │                             )                                            │ │
│ │                             (pooler): Sequential(                        │ │
│ │                           │   (layers.0): Linear(input_dims=768,         │ │
│ │                           output_dims=768, bias=True)                    │ │
│ │                           │   (layers.1): Tanh()                         │ │
│ │                             )                                            │ │
│ │                           )                                              │ │
│ │        cnn_kernel_sizes = '2,3,4,5'                                      │ │
│ │         cnn_num_filters = 128                                            │ │
│ │                  config = None                                           │ │
│ │        config_overrides = {}                                             │ │
│ │            config_table = <rich.table.Table object at 0x12efb0e50>       │ │
│ │          disable_mlflow = False                                          │ │
│ │ early_stopping_patience = 3                                              │ │
│ │ enable_dynamic_batching = True                                           │ │
│ │         eval_batch_size = 64                                             │ │
│ │              eval_steps = 50                                             │ │
│ │         experiment_name = 'titanic_modernbert'                           │ │
│ │   gradient_accumulation = 1                                              │ │
│ │           gradient_clip = 1.0                                            │ │
│ │         label_smoothing = 0.0                                            │ │
│ │           learning_rate = 2e-05                                          │ │
│ │          max_batch_size = 64                                             │ │
│ │   max_batch_size_config = 64                                             │ │
│ │              max_length = 256                                            │ │
│ │              model_desc = 'OptimizedModernBertMLX'                       │ │
│ │              model_name = 'answerdotai/ModernBERT-base'                  │ │
│ │              model_type = 'base'                                         │ │
│ │              num_epochs = 5                                              │ │
│ │             num_workers = 4                                              │ │
│ │              output_dir = PosixPath('output/standard_20250717_101821')   │ │
│ │           prefetch_size = 4                                              │ │
│ │             resume_from = None                                           │ │
│ │                 run_dir = PosixPath('output/standard_20250717_101821/ru… │ │
│ │                run_name = 'standard_20250717_101821'                     │ │
│ │              save_steps = 100                                            │ │
│ │               timestamp = '20250717_101826'                              │ │
│ │       tokenizer_backend = 'auto'                                         │ │
│ │            train_loader = <data.mlx_dataloader.KaggleDataLoader object   │ │
│ │                           at 0x12f43ad10>                                │ │
│ │              train_path = PosixPath('data/titanic/train.csv')            │ │
│ │           train_samples = 896                                            │ │
│ │         training_config = TrainingConfig(                                │ │
│ │                           │   epochs=5,                                  │ │
│ │                           │   batch_size=32,                             │ │
│ │                           │   learning_rate=2e-05,                       │ │
│ │                           │   warmup_steps=100,                          │ │
│ │                           │   max_steps=None,                            │ │
│ │                           │   model_name='answerdotai/ModernBERT-base',  │ │
│ │                           │   model_type='modernbert',                   │ │
│ │                           │   max_length=256,                            │ │
│ │                           │   num_labels=None,                           │ │
│ │                           │   train_path='data/titanic/train.csv',       │ │
│ │                           │   val_path='data/titanic/val.csv',           │ │
│ │                           │   test_path=None,                            │ │
│ │                           │   target_column=None,                        │ │
│ │                           │   optimizer=<OptimizerType.ADAMW: 'adamw'>,  │ │
│ │                           │                                              │ │
│ │                           lr_schedule=<LearningRateSchedule.COSINE_WARM… │ │
│ │                           'cosine_warmup'>,                              │ │
│ │                           │   loss_function=<LossFunction.CROSS_ENTROPY: │ │
│ │                           'cross_entropy'>,                              │ │
│ │                           │                                              │ │
│ │                           optimization_level=<OptimizationLevel.AUTO:    │ │
│ │                           'auto'>,                                       │ │
│ │                           │   memory=MemoryConfig(                       │ │
│ │                           │   │   enable_memory_profiling=True,          │ │
│ │                           │   │   memory_limit_gb=None,                  │ │
│ │                           │   │   dynamic_batch_sizing=True,             │ │
│ │                           │   │   min_batch_size=4,                      │ │
│ │                           │   │   max_batch_size=128,                    │ │
│ │                           │   │   memory_check_interval=100,             │ │
│ │                           │   │   unified_memory_fraction=0.8,           │ │
│ │                           │   │   enable_memory_pool=True,               │ │
│ │                           │   │   force_garbage_collection=True,         │ │
│ │                           │   │   gc_interval=500                        │ │
│ │                           │   ),                                         │ │
│ │                           │   mlx_optimization=MLXOptimizationConfig(    │ │
│ │                           │   │   enable_lazy_evaluation=True,           │ │
│ │                           │   │   eval_frequency=10,                     │ │
│ │                           │   │   enable_gradient_checkpointing=False,   │ │
│ │                           │   │   gradient_accumulation_steps=1,         │ │
│ │                           │   │   max_grad_norm=1.0,                     │ │
│ │                           │   │   device_placement_strategy='auto',      │ │
│ │                           │   │   enable_multi_device=False,             │ │
│ │                           │   │   mixed_precision=False,                 │ │
│ │                           │   │   precision_dtype='float32',             │ │
│ │                           │   │   enable_jit=True,                       │ │
│ │                           │   │   optimize_memory_layout=True            │ │
│ │                           │   ),                                         │ │
│ │                           │   monitoring=MonitoringConfig(               │ │
│ │                           │   │   enable_mlflow=True,                    │ │
│ │                           │   │   experiment_name='titanic_modernbert',  │ │
│ │                           │   │   run_name='standard_20250717_101821',   │ │
│ │                           │   │   tracking_uri=None,                     │ │
│ │                           │   │   log_level='INFO',                      │ │
│ │                           │   │   log_to_file=True,                      │ │
│ │                           │   │                                          │ │
│ │                           log_file_path='output/standard_20250717_10182… │ │
│ │                           │   │   enable_rich_console=False,             │ │
│ │                           │   │   log_frequency=10,                      │ │
│ │                           │   │   eval_frequency=500,                    │ │
│ │                           │   │   save_frequency=1000,                   │ │
│ │                           │   │   enable_progress_bar=True,              │ │
│ │                           │   │   progress_bar_style='rich',             │ │
│ │                           │   │   track_gradients=False,                 │ │
│ │                           │   │   track_weights=False,                   │ │
│ │                           │   │   track_memory=True,                     │ │
│ │                           │   │   track_performance=True                 │ │
│ │                           │   ),                                         │ │
│ │                           │   checkpoint=CheckpointConfig(               │ │
│ │                           │   │   enable_checkpointing=True,             │ │
│ │                           │   │                                          │ │
│ │                           checkpoint_dir='output/standard_20250717_1018… │ │
│ │                           │   │   checkpoint_frequency=100,              │ │
│ │                           │   │   save_optimizer_state=True,             │ │
│ │                           │   │   save_scheduler_state=True,             │ │
│ │                           │   │   save_random_state=True,                │ │
│ │                           │   │   save_model_weights=True,               │ │
│ │                           │   │   max_checkpoints_to_keep=5,             │ │
│ │                           │   │   save_best_model=True,                  │ │
│ │                           │   │   best_model_metric='val_accuracy',      │ │
│ │                           │   │   best_model_mode='max',                 │ │
│ │                           │   │   auto_resume=True,                      │ │
│ │                           │   │   resume_from_checkpoint=None,           │ │
│ │                           │   │   use_safetensors=True,                  │ │
│ │                           │   │   compress_checkpoints=False             │ │
│ │                           │   ),                                         │ │
│ │                           │   evaluation=EvaluationConfig(               │ │
│ │                           │   │   eval_during_training=True,             │ │
│ │                           │   │   eval_steps=50,                         │ │
│ │                           │   │   eval_strategy='steps',                 │ │
│ │                           │   │   primary_metric='accuracy',             │ │
│ │                           │   │   metrics_to_compute=[                   │ │
│ │                           │   │   │   'accuracy',                        │ │
│ │                           │   │   │   'precision',                       │ │
│ │                           │   │   │   'recall',                          │ │
│ │                           │   │   │   'f1',                              │ │
│ │                           │   │   │   'auc'                              │ │
│ │                           │   │   ],                                     │ │
│ │                           │   │   enable_early_stopping=True,            │ │
│ │                           │   │   early_stopping_patience=3,             │ │
│ │                           │   │   early_stopping_threshold=0.001,        │ │
│ │                           │   │   early_stopping_metric='val_loss',      │ │
│ │                           │   │   early_stopping_mode='min',             │ │
│ │                           │   │   validation_split=0.2,                  │ │
│ │                           │   │   validation_batch_size=None,            │ │
│ │                           │   │   test_at_end=True,                      │ │
│ │                           │   │   generate_predictions=True,             │ │
│ │                           │   │   save_predictions=True                  │ │
│ │                           │   ),                                         │ │
│ │                           │   advanced=AdvancedFeatures(                 │ │
│ │                           │   │   label_smoothing=0.0,                   │ │
│ │                           │   │   dropout_rate=0.1,                      │ │
│ │                           │   │   weight_decay=0.01,                     │ │
│ │                           │   │   enable_augmentation=True,              │ │
│ │                           │   │   augmentation_probability=0.5,          │ │
│ │                           │   │   enable_curriculum_learning=False,      │ │
│ │                           │   │   curriculum_strategy='difficulty',      │ │
│ │                           │   │   curriculum_pace=0.1,                   │ │
│ │                           │   │   enable_ensembling=False,               │ │
│ │                           │   │   ensemble_size=1,                       │ │
│ │                           │   │   ensemble_strategy='averaging',         │ │
│ │                           │   │   enable_distillation=False,             │ │
│ │                           │   │   teacher_model_path=None,               │ │
│ │                           │   │   distillation_temperature=3.0,          │ │
│ │                           │   │   distillation_alpha=0.5,                │ │
│ │                           │   │   enable_hpo=False,                      │ │
│ │                           │   │   hpo_backend='optuna',                  │ │
│ │                           │   │   hpo_trials=50,                         │ │
│ │                           │   │   hpo_metric='val_accuracy'              │ │
│ │                           │   ),                                         │ │
│ │                           │   seed=42,                                   │ │
│ │                           │   deterministic=True,                        │ │
│ │                           │                                              │ │
│ │                           output_dir='output/standard_20250717_101821/r… │ │
│ │                           │   experiment_name='titanic_modernbert',      │ │
│ │                           │   run_name='standard_20250717_101821'        │ │
│ │                           )                                              │ │
│ │        use_dilated_conv = True                                           │ │
│ │      use_mlx_embeddings = False                                          │ │
│ │              val_loader = <data.mlx_dataloader.KaggleDataLoader object   │ │
│ │                           at 0x119b7edd0>                                │ │
│ │                val_path = PosixPath('data/titanic/val.csv')              │ │
│ │             val_samples = 192                                            │ │
│ │            warmup_ratio = 0.7194244604316546                             │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /Users/parkergabel/PycharmProjects/bert-playground/models/classification/tit │
│ anic_classifier.py:53 in __init__                                            │
│                                                                              │
│    50 │   │                                                                  │
│    51 │   │   # Initialize classification head                               │
│    52 │   │   self.classification_head = BinaryClassificationHead(           │
│ ❱  53 │   │   │   input_dim=embedding_model.hidden_size,                     │
│    54 │   │   │   hidden_dim=hidden_dim,                                     │
│    55 │   │   │   dropout_prob=dropout_prob,                                 │
│    56 │   │   │   use_layer_norm=use_layer_norm,                             │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │        activation = 'relu'                                               │ │
│ │      dropout_prob = 0.1                                                  │ │
│ │   embedding_model = ModernBertModel(                                     │ │
│ │                       (embeddings): OptimizedEmbeddings(                 │ │
│ │                     │   (word_embeddings): Embedding(50368, 768)         │ │
│ │                     │   (position_embeddings): Embedding(8192, 768)      │ │
│ │                     │   (token_type_embeddings): Embedding(2, 768)       │ │
│ │                     │   (LayerNorm): LayerNorm(768, eps=1e-12,           │ │
│ │                     affine=True)                                         │ │
│ │                     │   (dropout): Dropout(p=0.09999999999999998)        │ │
│ │                       )                                                  │ │
│ │                       (encoder): Sequential(                             │ │
│ │                     │   (layers.0): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.1): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.2): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.3): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.4): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.5): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.6): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.7): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.8): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.9): TransformerBlock(                    │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.10): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.11): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.12): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.13): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.14): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.15): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.16): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.17): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.18): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.19): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.20): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                     │   (layers.21): TransformerBlock(                   │ │
│ │                     │     (attention): FusedMultiHeadAttention(          │ │
│ │                     │   │   (qkv_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │   (out_proj): Linear(input_dims=768,           │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (dropout): Dropout(p=0.09999999999999998)    │ │
│ │                     │     )                                              │ │
│ │                     │     (attention_norm): LayerNorm(768, eps=1e-12,    │ │
│ │                     affine=True)                                         │ │
│ │                     │     (mlp): Sequential(                             │ │
│ │                     │   │   (layers.0): Linear(input_dims=768,           │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │   (layers.1): GELU()                           │ │
│ │                     │   │   (layers.2): Dropout(p=0.09999999999999998)   │ │
│ │                     │   │   (layers.3): Linear(input_dims=3072,          │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │   (layers.4): Dropout(p=0.09999999999999998)   │ │
│ │                     │     )                                              │ │
│ │                     │     (mlp_norm): LayerNorm(768, eps=1e-12,          │ │
│ │                     affine=True)                                         │ │
│ │                     │   )                                                │ │
│ │                       )                                                  │ │
│ │                       (pooler): Sequential(                              │ │
│ │                     │   (layers.0): Linear(input_dims=768,               │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   (layers.1): Tanh()                               │ │
│ │                       )                                                  │ │
│ │                     )                                                    │ │
│ │ freeze_embeddings = False                                                │ │
│ │        hidden_dim = None                                                 │ │
│ │              self = TitanicClassifier(                                   │ │
│ │                       (embedding_model): ModernBertModel(                │ │
│ │                     │   (embeddings): OptimizedEmbeddings(               │ │
│ │                     │     (word_embeddings): Embedding(50368, 768)       │ │
│ │                     │     (position_embeddings): Embedding(8192, 768)    │ │
│ │                     │     (token_type_embeddings): Embedding(2, 768)     │ │
│ │                     │     (LayerNorm): LayerNorm(768, eps=1e-12,         │ │
│ │                     affine=True)                                         │ │
│ │                     │     (dropout): Dropout(p=0.09999999999999998)      │ │
│ │                     │   )                                                │ │
│ │                     │   (encoder): Sequential(                           │ │
│ │                     │     (layers.0): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.1): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.2): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.3): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.4): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.5): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.6): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.7): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.8): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.9): TransformerBlock(                  │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.10): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.11): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.12): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.13): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.14): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.15): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.16): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.17): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.18): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.19): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.20): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │     (layers.21): TransformerBlock(                 │ │
│ │                     │   │   (attention): FusedMultiHeadAttention(        │ │
│ │                     │   │     (qkv_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=2304, bias=True)                         │ │
│ │                     │   │     (out_proj): Linear(input_dims=768,         │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (dropout): Dropout(p=0.09999999999999998)  │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (attention_norm): LayerNorm(768, eps=1e-12,  │ │
│ │                     affine=True)                                         │ │
│ │                     │   │   (mlp): Sequential(                           │ │
│ │                     │   │     (layers.0): Linear(input_dims=768,         │ │
│ │                     output_dims=3072, bias=True)                         │ │
│ │                     │   │     (layers.1): GELU()                         │ │
│ │                     │   │     (layers.2): Dropout(p=0.09999999999999998) │ │
│ │                     │   │     (layers.3): Linear(input_dims=3072,        │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │   │     (layers.4): Dropout(p=0.09999999999999998) │ │
│ │                     │   │   )                                            │ │
│ │                     │   │   (mlp_norm): LayerNorm(768, eps=1e-12,        │ │
│ │                     affine=True)                                         │ │
│ │                     │     )                                              │ │
│ │                     │   )                                                │ │
│ │                     │   (pooler): Sequential(                            │ │
│ │                     │     (layers.0): Linear(input_dims=768,             │ │
│ │                     output_dims=768, bias=True)                          │ │
│ │                     │     (layers.1): Tanh()                             │ │
│ │                     │   )                                                │ │
│ │                       )                                                  │ │
│ │                     )                                                    │ │
│ │    use_layer_norm = False                                                │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /Users/parkergabel/PycharmProjects/bert-playground/.venv/lib/python3.10/site │
│ -packages/mlx/nn/layers/base.py:103 in __getattr__                           │
│                                                                              │
│   100 │   │   if (value := self.get(key, None)) is not None:                 │
│   101 │   │   │   return value                                               │
│   102 │   │   else:                                                          │
│ ❱ 103 │   │   │   super(Module, self).__getattribute__(key)                  │
│   104 │                                                                      │
│   105 │   def __setattr__(self, key: str, val: Any):                         │
│   106 │   │   if isinstance(val, (mx.array, dict, list, tuple)):             │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │   key = 'hidden_size'                                                    │ │
│ │  self = ModernBertModel(                                                 │ │
│ │           (embeddings): OptimizedEmbeddings(                             │ │
│ │         │   (word_embeddings): Embedding(50368, 768)                     │ │
│ │         │   (position_embeddings): Embedding(8192, 768)                  │ │
│ │         │   (token_type_embeddings): Embedding(2, 768)                   │ │
│ │         │   (LayerNorm): LayerNorm(768, eps=1e-12, affine=True)          │ │
│ │         │   (dropout): Dropout(p=0.09999999999999998)                    │ │
│ │           )                                                              │ │
│ │           (encoder): Sequential(                                         │ │
│ │         │   (layers.0): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.1): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.2): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.3): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.4): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.5): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.6): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.7): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.8): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.9): TransformerBlock(                                │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.10): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.11): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.12): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.13): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.14): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.15): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.16): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.17): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.18): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.19): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.20): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │         │   (layers.21): TransformerBlock(                               │ │
│ │         │     (attention): FusedMultiHeadAttention(                      │ │
│ │         │   │   (qkv_proj): Linear(input_dims=768, output_dims=2304,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (out_proj): Linear(input_dims=768, output_dims=768,      │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (dropout): Dropout(p=0.09999999999999998)                │ │
│ │         │     )                                                          │ │
│ │         │     (attention_norm): LayerNorm(768, eps=1e-12, affine=True)   │ │
│ │         │     (mlp): Sequential(                                         │ │
│ │         │   │   (layers.0): Linear(input_dims=768, output_dims=3072,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.1): GELU()                                       │ │
│ │         │   │   (layers.2): Dropout(p=0.09999999999999998)               │ │
│ │         │   │   (layers.3): Linear(input_dims=3072, output_dims=768,     │ │
│ │         bias=True)                                                       │ │
│ │         │   │   (layers.4): Dropout(p=0.09999999999999998)               │ │
│ │         │     )                                                          │ │
│ │         │     (mlp_norm): LayerNorm(768, eps=1e-12, affine=True)         │ │
│ │         │   )                                                            │ │
│ │           )                                                              │ │
│ │           (pooler): Sequential(                                          │ │
│ │         │   (layers.0): Linear(input_dims=768, output_dims=768,          │ │
│ │         bias=True)                                                       │ │
│ │         │   (layers.1): Tanh()                                           │ │
│ │           )                                                              │ │
│ │         )                                                                │ │
│ │ value = None                                                             │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
╰──────────────────────────────────────────────────────────────────────────────╯
AttributeError: 'ModernBertModel' object has no attribute 'hidden_size'

============================================================
Production Training: STANDARD Configuration
============================================================
Description: Standard training (5 epochs, balanced settings)
Dataset: Titanic (891 training samples, ~2673 with augmentation)
Configuration:
  batch_size: 32
  num_epochs: 5
  learning_rate: 2e-05
  warmup_steps: 100
  eval_steps: 50
Output directory: ./output/standard_20250717_101821
MLflow tracking: Enabled
============================================================

Expected training steps: 415 (83 per epoch)
Estimated time: 207.5 - 622.5 seconds

Starting training...

Training failed with exit code 1
